{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a2cd120-8e55-4512-bdcb-e5bac1680ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data directory: D:\\.Study\\projects\\EnergyForecasting\\data\\processed\n",
      "Model-ready data directory: D:\\.Study\\projects\\EnergyForecasting\\data\\model_ready\n",
      "Zones being processed: ['DOM', 'PN', 'PEPCO', 'AECO', 'PE']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib # Added for saving scaler, though it was in cell 6 before\n",
    "\n",
    "# --- Configuration ---\n",
    "# Directory where your PROCESSED load and weather files are stored\n",
    "processed_data_dir = r'D:\\.Study\\projects\\EnergyForecasting\\data\\processed' # !!! UPDATE THIS PATH IF NEEDED !!!\n",
    "\n",
    "# --- NEW FILENAMES (from your multi-year processing notebooks) ---\n",
    "# Make sure these EXACTLY match the output filenames from your\n",
    "# 02-load-data-processing and 03-weather-data-processing notebooks.\n",
    "processed_load_filename_MULTIYEAR = 'pjm_hourly_load_multi_zone_cleaned_2022-2025May.csv'\n",
    "processed_weather_filename_MULTIYEAR = 'pjm_hourly_weather_multi_zone_cleaned_2022-2025May.csv'\n",
    "\n",
    "# Directory to save the final, model-ready datasets\n",
    "model_ready_data_dir = r'D:\\.Study\\projects\\EnergyForecasting\\src\\model_ready' # !!! UPDATE THIS PATH IF NEEDED !!!\n",
    "\n",
    "# List of your PJM zones (ensure this is consistent with previous notebooks)\n",
    "# Example: ['DOM', 'PN', 'PEPCO', 'AECO', 'PE']\n",
    "PJM_ZONES_UPPER = ['DOM', 'PN', 'PEPCO', 'AECO', 'PE'] # !!! UPDATE TO MATCH YOUR ZONES !!!\n",
    "\n",
    "# Define target columns (load for each zone)\n",
    "target_column_template = '{ZONE}_Load'\n",
    "\n",
    "print(f\"Processed data directory: {processed_data_dir}\")\n",
    "print(f\"Model-ready data directory: {model_ready_data_dir}\")\n",
    "print(f\"Zones being processed: {PJM_ZONES_UPPER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8779d08-0aeb-4d6b-bef1-7f5e7e649173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded load data: D:\\.Study\\projects\\EnergyForecasting\\data\\processed\\pjm_hourly_load_multi_zone_cleaned_2022-2025May.csv\n",
      "  Load data shape: (29879, 5)\n",
      "\n",
      "Successfully loaded weather data: D:\\.Study\\projects\\EnergyForecasting\\data\\processed\\pjm_hourly_weather_multi_zone_cleaned_2022-2025May.csv\n",
      "  Weather data shape: (29928, 55)\n",
      "Localizing load_df index to UTC...\n",
      "Weather_df index timezone is already UTC: UTC\n"
     ]
    }
   ],
   "source": [
    "# Construct full file paths using the new multi-year filenames\n",
    "load_data_path = os.path.join(processed_data_dir, processed_load_filename_MULTIYEAR)\n",
    "weather_data_path = os.path.join(processed_data_dir, processed_weather_filename_MULTIYEAR)\n",
    "\n",
    "load_df = pd.DataFrame() # Initialize as empty\n",
    "weather_df = pd.DataFrame() # Initialize as empty\n",
    "\n",
    "# Load the datasets\n",
    "try:\n",
    "    load_df = pd.read_csv(load_data_path, index_col=0, parse_dates=True)\n",
    "    print(f\"Successfully loaded load data: {load_data_path}\")\n",
    "    print(f\"  Load data shape: {load_df.shape}\")\n",
    "    # print(load_df.head(3)) # Keep for verification if needed\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Load data file not found at {load_data_path}\")\n",
    "    print(\"Please ensure 02-load-data-processing notebook has run successfully and saved the file with the correct name.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading load data from {load_data_path}: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    weather_df = pd.read_csv(weather_data_path, index_col=0, parse_dates=True)\n",
    "    print(f\"\\nSuccessfully loaded weather data: {weather_data_path}\")\n",
    "    print(f\"  Weather data shape: {weather_df.shape}\")\n",
    "    # print(weather_df.head(3)) # Keep for verification if needed\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: Weather data file not found at {weather_data_path}\")\n",
    "    print(\"Please ensure 03-weather-data-processing notebook has run successfully and saved the file with the correct name.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR loading weather data from {weather_data_path}: {e}\")\n",
    "\n",
    "\n",
    "# Ensure indices are timezone-aware (UTC) if they aren't already\n",
    "# The previous processing steps should have handled this, but good to double-check.\n",
    "if not load_df.empty:\n",
    "    if load_df.index.tz is None:\n",
    "        print(\"Localizing load_df index to UTC...\")\n",
    "        load_df.index = load_df.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')\n",
    "    elif str(load_df.index.tz).upper() != 'UTC':\n",
    "        print(f\"Converting load_df index from {load_df.index.tz} to UTC...\")\n",
    "        load_df.index = load_df.index.tz_convert('UTC')\n",
    "    else:\n",
    "        print(f\"Load_df index timezone is already UTC: {load_df.index.tz}\")\n",
    "\n",
    "if not weather_df.empty:\n",
    "    if weather_df.index.tz is None:\n",
    "        print(\"Localizing weather_df index to UTC...\")\n",
    "        weather_df.index = weather_df.index.tz_localize('UTC', ambiguous='NaT', nonexistent='NaT')\n",
    "    elif str(weather_df.index.tz).upper() != 'UTC':\n",
    "        print(f\"Converting weather_df index from {weather_df.index.tz} to UTC...\")\n",
    "        weather_df.index = weather_df.index.tz_convert('UTC')\n",
    "    else:\n",
    "        print(f\"Weather_df index timezone is already UTC: {weather_df.index.tz}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9f51d4c-7e00-4438-81f4-0a487aaf56b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Aligning and Merging DataFrames ---\n",
      "  Ensured load_df and weather_df indices are sorted.\n",
      "  Number of common timestamps found: 29879\n",
      "  Load data shape after alignment: (29879, 5)\n",
      "  Weather data shape after alignment: (29879, 55)\n",
      "\n",
      "Combined DataFrame (first 3 rows):\n",
      "                           DOM_Load   PN_Load  PEPCO_Load  AECO_Load  \\\n",
      "2022-01-01 05:00:00+00:00  9879.540  1517.266    2169.749    871.977   \n",
      "2022-01-01 06:00:00+00:00  9549.307  1462.484    2065.314    834.028   \n",
      "2022-01-01 07:00:00+00:00  9346.150  1434.579    2013.883    796.671   \n",
      "\n",
      "                            PE_Load  DOM_Temp_C  DOM_DewPoint_C  \\\n",
      "2022-01-01 05:00:00+00:00  3490.865        -7.8           -14.4   \n",
      "2022-01-01 06:00:00+00:00  3352.814        -6.0           -15.3   \n",
      "2022-01-01 07:00:00+00:00  3224.528        -4.7           -15.1   \n",
      "\n",
      "                           DOM_AppTemp_C  DOM_Precip_mm  DOM_WeatherCode  ...  \\\n",
      "2022-01-01 05:00:00+00:00          -12.4            0.0              3.0  ...   \n",
      "2022-01-01 06:00:00+00:00          -10.6            0.0              3.0  ...   \n",
      "2022-01-01 07:00:00+00:00           -9.0            0.0              3.0  ...   \n",
      "\n",
      "                           PE_DewPoint_C  PE_AppTemp_C  PE_Precip_mm  \\\n",
      "2022-01-01 05:00:00+00:00            8.5           7.8           0.0   \n",
      "2022-01-01 06:00:00+00:00            8.6           8.2           0.1   \n",
      "2022-01-01 07:00:00+00:00            8.8           7.4           0.0   \n",
      "\n",
      "                           PE_WeatherCode  PE_CloudCover_Percent  \\\n",
      "2022-01-01 05:00:00+00:00             3.0                  100.0   \n",
      "2022-01-01 06:00:00+00:00            51.0                  100.0   \n",
      "2022-01-01 07:00:00+00:00             3.0                  100.0   \n",
      "\n",
      "                           PE_WindSpeed_kmh  PE_SolarRad_Wm2  PE_IsDay  \\\n",
      "2022-01-01 05:00:00+00:00               7.6              0.0         0   \n",
      "2022-01-01 06:00:00+00:00               4.8              0.0         0   \n",
      "2022-01-01 07:00:00+00:00              10.3              0.0         0   \n",
      "\n",
      "                           PE_WindGust_kmh  PE_Snowfall_cm  \n",
      "2022-01-01 05:00:00+00:00             14.8             0.0  \n",
      "2022-01-01 06:00:00+00:00             12.2             0.0  \n",
      "2022-01-01 07:00:00+00:00             16.6             0.0  \n",
      "\n",
      "[3 rows x 60 columns]\n",
      "\n",
      "Information about Combined DataFrame:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 29879 entries, 2022-01-01 05:00:00+00:00 to 2025-05-30 03:00:00+00:00\n",
      "Data columns (total 60 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   DOM_Load                  29879 non-null  float64\n",
      " 1   PN_Load                   29879 non-null  float64\n",
      " 2   PEPCO_Load                29879 non-null  float64\n",
      " 3   AECO_Load                 29879 non-null  float64\n",
      " 4   PE_Load                   29879 non-null  float64\n",
      " 5   DOM_Temp_C                29879 non-null  float64\n",
      " 6   DOM_DewPoint_C            29879 non-null  float64\n",
      " 7   DOM_AppTemp_C             29879 non-null  float64\n",
      " 8   DOM_Precip_mm             29879 non-null  float64\n",
      " 9   DOM_WeatherCode           29879 non-null  float64\n",
      " 10  DOM_CloudCover_Percent    29879 non-null  float64\n",
      " 11  DOM_WindSpeed_kmh         29879 non-null  float64\n",
      " 12  DOM_SolarRad_Wm2          29879 non-null  float64\n",
      " 13  DOM_IsDay                 29879 non-null  int64  \n",
      " 14  DOM_WindGust_kmh          29879 non-null  float64\n",
      " 15  DOM_Snowfall_cm           29879 non-null  float64\n",
      " 16  PN_Temp_C                 29879 non-null  float64\n",
      " 17  PN_DewPoint_C             29879 non-null  float64\n",
      " 18  PN_AppTemp_C              29879 non-null  float64\n",
      " 19  PN_Precip_mm              29879 non-null  float64\n",
      " 20  PN_WeatherCode            29879 non-null  float64\n",
      " 21  PN_CloudCover_Percent     29879 non-null  float64\n",
      " 22  PN_WindSpeed_kmh          29879 non-null  float64\n",
      " 23  PN_SolarRad_Wm2           29879 non-null  float64\n",
      " 24  PN_IsDay                  29879 non-null  int64  \n",
      " 25  PN_WindGust_kmh           29879 non-null  float64\n",
      " 26  PN_Snowfall_cm            29879 non-null  float64\n",
      " 27  PEPCO_Temp_C              29879 non-null  float64\n",
      " 28  PEPCO_DewPoint_C          29879 non-null  float64\n",
      " 29  PEPCO_AppTemp_C           29879 non-null  float64\n",
      " 30  PEPCO_Precip_mm           29879 non-null  float64\n",
      " 31  PEPCO_WeatherCode         29879 non-null  float64\n",
      " 32  PEPCO_CloudCover_Percent  29879 non-null  float64\n",
      " 33  PEPCO_WindSpeed_kmh       29879 non-null  float64\n",
      " 34  PEPCO_SolarRad_Wm2        29879 non-null  float64\n",
      " 35  PEPCO_IsDay               29879 non-null  int64  \n",
      " 36  PEPCO_WindGust_kmh        29879 non-null  float64\n",
      " 37  PEPCO_Snowfall_cm         29879 non-null  float64\n",
      " 38  AECO_Temp_C               29879 non-null  float64\n",
      " 39  AECO_DewPoint_C           29879 non-null  float64\n",
      " 40  AECO_AppTemp_C            29879 non-null  float64\n",
      " 41  AECO_Precip_mm            29879 non-null  float64\n",
      " 42  AECO_WeatherCode          29879 non-null  float64\n",
      " 43  AECO_CloudCover_Percent   29879 non-null  float64\n",
      " 44  AECO_WindSpeed_kmh        29879 non-null  float64\n",
      " 45  AECO_SolarRad_Wm2         29879 non-null  float64\n",
      " 46  AECO_IsDay                29879 non-null  int64  \n",
      " 47  AECO_WindGust_kmh         29879 non-null  float64\n",
      " 48  AECO_Snowfall_cm          29879 non-null  float64\n",
      " 49  PE_Temp_C                 29879 non-null  float64\n",
      " 50  PE_DewPoint_C             29879 non-null  float64\n",
      " 51  PE_AppTemp_C              29879 non-null  float64\n",
      " 52  PE_Precip_mm              29879 non-null  float64\n",
      " 53  PE_WeatherCode            29879 non-null  float64\n",
      " 54  PE_CloudCover_Percent     29879 non-null  float64\n",
      " 55  PE_WindSpeed_kmh          29879 non-null  float64\n",
      " 56  PE_SolarRad_Wm2           29879 non-null  float64\n",
      " 57  PE_IsDay                  29879 non-null  int64  \n",
      " 58  PE_WindGust_kmh           29879 non-null  float64\n",
      " 59  PE_Snowfall_cm            29879 non-null  float64\n",
      "dtypes: float64(55), int64(5)\n",
      "memory usage: 13.9 MB\n",
      "\n",
      "No NaNs in combined_df after merge.\n"
     ]
    }
   ],
   "source": [
    "combined_df = pd.DataFrame() # Initialize as empty\n",
    "\n",
    "if not load_df.empty and not weather_df.empty:\n",
    "    print(\"\\n--- Aligning and Merging DataFrames ---\")\n",
    "    \n",
    "    # Ensure both DataFrames have sorted indices before intersection for robustness\n",
    "    load_df.sort_index(inplace=True)\n",
    "    weather_df.sort_index(inplace=True)\n",
    "    print(\"  Ensured load_df and weather_df indices are sorted.\")\n",
    "\n",
    "    # Find the intersection of the two DatetimeIndexes\n",
    "    common_index = load_df.index.intersection(weather_df.index)\n",
    "    print(f\"  Number of common timestamps found: {len(common_index)}\")\n",
    "    \n",
    "    if len(common_index) == 0:\n",
    "        print(\"  ERROR: No common timestamps between load and weather data. Cannot merge.\")\n",
    "        print(f\"    Load data index range: {load_df.index.min()} to {load_df.index.max()}\")\n",
    "        print(f\"    Weather data index range: {weather_df.index.min()} to {weather_df.index.max()}\")\n",
    "    else:\n",
    "        load_df_aligned = load_df.loc[common_index]\n",
    "        weather_df_aligned = weather_df.loc[common_index]\n",
    "        \n",
    "        print(f\"  Load data shape after alignment: {load_df_aligned.shape}\")\n",
    "        print(f\"  Weather data shape after alignment: {weather_df_aligned.shape}\")\n",
    "        \n",
    "        combined_df = pd.concat([load_df_aligned, weather_df_aligned], axis=1)\n",
    "        \n",
    "        print(\"\\nCombined DataFrame (first 3 rows):\")\n",
    "        print(combined_df.head(3))\n",
    "        print(\"\\nInformation about Combined DataFrame:\")\n",
    "        combined_df.info()\n",
    "        \n",
    "        # Check for NaNs after merge (should ideally be 0 if common_index is used correctly)\n",
    "        nans_after_merge = combined_df.isnull().sum().sum()\n",
    "        if nans_after_merge > 0:\n",
    "            print(f\"\\nTotal NaNs in combined_df after merge: {nans_after_merge}\")\n",
    "            print(\"  This might indicate issues if load/weather data had internal NaNs not filled, or column name clashes.\")\n",
    "            print(\"  Attempting ffill and bfill...\")\n",
    "            combined_df.ffill(inplace=True)\n",
    "            combined_df.bfill(inplace=True)\n",
    "            print(f\"  Total NaNs after fill: {combined_df.isnull().sum().sum()}\")\n",
    "        else:\n",
    "            print(f\"\\nNo NaNs in combined_df after merge.\")\n",
    "else:\n",
    "    print(\"One or both DataFrames (load_df, weather_df) are empty. Cannot merge.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc054a6e-a1af-484b-bc40-f57d8a5eade3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Feature Engineering: Adding Time-Based Features ---\n",
      "Added basic time features (hour, day, month, year, etc.).\n",
      "\n",
      "Adding lagged load features...\n",
      "  Added lagged features for DOM_Load\n",
      "  Added lagged features for PN_Load\n",
      "  Added lagged features for PEPCO_Load\n",
      "  Added lagged features for AECO_Load\n",
      "  Added lagged features for PE_Load\n",
      "\n",
      "Adding rolling mean weather features (e.g., Temp_C)...\n",
      "  Added rolling mean for DOM_Temp_C\n",
      "  Added rolling mean for PN_Temp_C\n",
      "  Added rolling mean for PEPCO_Temp_C\n",
      "  Added rolling mean for AECO_Temp_C\n",
      "  Added rolling mean for PE_Temp_C\n",
      "\n",
      "Dropped 168 rows due to NaNs from new lagged/rolling features.\n",
      "\n",
      "Adding cyclical time features...\n",
      "Added cyclical time features.\n",
      "\n",
      "New shape of combined_df after all feature engineering: (29711, 94)\n"
     ]
    }
   ],
   "source": [
    "if not combined_df.empty:\n",
    "    print(\"\\n--- Feature Engineering: Adding Time-Based Features ---\")\n",
    "    \n",
    "    if not isinstance(combined_df.index, pd.DatetimeIndex):\n",
    "        print(\"ERROR: combined_df.index is not a DatetimeIndex. Cannot create time features.\")\n",
    "    else:\n",
    "        # Basic time features\n",
    "        combined_df['hour_of_day'] = combined_df.index.hour\n",
    "        combined_df['day_of_week'] = combined_df.index.dayofweek # Monday=0, Sunday=6\n",
    "        combined_df['day_of_year'] = combined_df.index.dayofyear\n",
    "        combined_df['month'] = combined_df.index.month\n",
    "        combined_df['week_of_year'] = combined_df.index.isocalendar().week.astype(int)\n",
    "        combined_df['year'] = combined_df.index.year\n",
    "        print(\"Added basic time features (hour, day, month, year, etc.).\")\n",
    "\n",
    "        # --- ADDING LAGGED LOAD FEATURES ---\n",
    "        print(\"\\nAdding lagged load features...\")\n",
    "        lag_periods_hours = {\n",
    "            'lag_24h': 24,        # Previous day, same hour\n",
    "            'lag_48h': 48,        # Two days ago, same hour\n",
    "            'lag_168h': 24 * 7    # Previous week, same hour\n",
    "        }\n",
    "        for zone in PJM_ZONES_UPPER:\n",
    "            load_col_name = target_column_template.replace('{ZONE}', zone)\n",
    "            if load_col_name in combined_df.columns:\n",
    "                for lag_name, hours in lag_periods_hours.items():\n",
    "                    combined_df[f'{load_col_name}_{lag_name}'] = combined_df[load_col_name].shift(hours)\n",
    "                print(f\"  Added lagged features for {load_col_name}\")\n",
    "            else:\n",
    "                print(f\"  WARNING: Load column {load_col_name} not found for adding lags.\")\n",
    "        \n",
    "        # --- ADDING ROLLING MEAN WEATHER FEATURES (Example for Temperature) ---\n",
    "        # This adds complexity but can be useful. You can expand to other weather vars.\n",
    "        print(\"\\nAdding rolling mean weather features (e.g., Temp_C)...\")\n",
    "        rolling_window_hours = 3 # 3-hour rolling average\n",
    "        for zone in PJM_ZONES_UPPER:\n",
    "            temp_col_name = f\"{zone}_Temp_C\" # Assuming this is your temp column name structure\n",
    "            if temp_col_name in combined_df.columns:\n",
    "                combined_df[f'{temp_col_name}_roll_mean_{rolling_window_hours}h'] = combined_df[temp_col_name].rolling(window=rolling_window_hours, min_periods=1).mean()\n",
    "                # Shift by 1 because rolling mean includes current hour; we want mean of *past* hours as feature\n",
    "                combined_df[f'{temp_col_name}_roll_mean_{rolling_window_hours}h'] = combined_df[f'{temp_col_name}_roll_mean_{rolling_window_hours}h'].shift(1)\n",
    "                print(f\"  Added rolling mean for {temp_col_name}\")\n",
    "\n",
    "\n",
    "        # Handle NaNs created by shift() and rolling().shift() operations\n",
    "        # This is important BEFORE creating cyclical features from any columns that might be all NaN at the start\n",
    "        initial_len = len(combined_df)\n",
    "        combined_df.dropna(inplace=True) # Drop rows with NaNs from lags/rolling\n",
    "        print(f\"\\nDropped {initial_len - len(combined_df)} rows due to NaNs from new lagged/rolling features.\")\n",
    "        if combined_df.empty:\n",
    "            print(\"ERROR: DataFrame became empty after dropping NaNs from feature engineering. Check lag periods or data length.\")\n",
    "        else:\n",
    "            # Cyclical features (useful for models that don't inherently understand circularity)\n",
    "            print(\"\\nAdding cyclical time features...\")\n",
    "            # Hour\n",
    "            combined_df['hour_sin'] = np.sin(2 * np.pi * combined_df['hour_of_day'] / 24.0)\n",
    "            combined_df['hour_cos'] = np.cos(2 * np.pi * combined_df['hour_of_day'] / 24.0)\n",
    "            # Day of Week\n",
    "            combined_df['day_of_week_sin'] = np.sin(2 * np.pi * combined_df['day_of_week'] / 7.0)\n",
    "            combined_df['day_of_week_cos'] = np.cos(2 * np.pi * combined_df['day_of_week'] / 7.0)\n",
    "            # Month\n",
    "            combined_df['month_sin'] = np.sin(2 * np.pi * combined_df['month'] / 12.0)\n",
    "            combined_df['month_cos'] = np.cos(2 * np.pi * combined_df['month'] / 12.0)\n",
    "            # Day of Year\n",
    "            combined_df['day_of_year_sin'] = np.sin(2 * np.pi * combined_df['day_of_year'] / 365.25)\n",
    "            combined_df['day_of_year_cos'] = np.cos(2 * np.pi * combined_df['day_of_year'] / 365.25)\n",
    "            print(\"Added cyclical time features.\")\n",
    "            # print(combined_df[['hour_of_day', 'hour_sin', 'hour_cos']].head(3)) # Sample check\n",
    "            print(f\"\\nNew shape of combined_df after all feature engineering: {combined_df.shape}\")\n",
    "else:\n",
    "    print(\"Combined DataFrame is empty. Skipping feature engineering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a022ffc7-8ac0-4680-ab71-30bc80c5f834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Splitting Data into Train, Validation, and Test Sets ---\n",
      "Total samples in combined_df: 29711\n",
      "Training set shape:   (20797, 94), Index: 2022-01-08 05:00:00+00:00 to 2024-05-23 17:00:00+00:00\n",
      "Validation set shape: (4456, 94), Index: 2024-05-23 18:00:00+00:00 to 2024-11-25 09:00:00+00:00\n",
      "Test set shape:       (4458, 94), Index: 2024-11-25 10:00:00+00:00 to 2025-05-30 03:00:00+00:00\n",
      "Data splits are chronological and non-overlapping.\n"
     ]
    }
   ],
   "source": [
    "train_df, val_df, test_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame() # Initialize\n",
    "\n",
    "if not combined_df.empty:\n",
    "    print(\"\\n--- Splitting Data into Train, Validation, and Test Sets ---\")\n",
    "    n = len(combined_df)\n",
    "    if n < 24*7*4 : # Arbitrary check for minimum data length\n",
    "        print(f\"WARNING: Combined DataFrame has only {n} samples. Splits might be very small.\")\n",
    "\n",
    "    # Define split percentages\n",
    "    train_split_pct = 0.7  # 70% for training\n",
    "    val_split_pct = 0.15   # Next 15% for validation\n",
    "    # Test split will be the remaining 15%\n",
    "    \n",
    "    train_end_idx = int(n * train_split_pct)\n",
    "    val_end_idx = train_end_idx + int(n * val_split_pct) # Calculate val_end_idx based on train_end_idx\n",
    "    \n",
    "    train_df = combined_df.iloc[:train_end_idx].copy()\n",
    "    val_df = combined_df.iloc[train_end_idx:val_end_idx].copy()\n",
    "    test_df = combined_df.iloc[val_end_idx:].copy()\n",
    "    \n",
    "    print(f\"Total samples in combined_df: {n}\")\n",
    "    if not train_df.empty:\n",
    "        print(f\"Training set shape:   {train_df.shape}, Index: {train_df.index.min()} to {train_df.index.max()}\")\n",
    "    else: print(\"Training set is empty.\")\n",
    "    if not val_df.empty:\n",
    "        print(f\"Validation set shape: {val_df.shape}, Index: {val_df.index.min()} to {val_df.index.max()}\")\n",
    "    else: print(\"Validation set is empty.\")\n",
    "    if not test_df.empty:\n",
    "        print(f\"Test set shape:       {test_df.shape}, Index: {test_df.index.min()} to {test_df.index.max()}\")\n",
    "    else: print(\"Test set is empty.\")\n",
    "    \n",
    "    # Verify no overlap in indices if all sets are non-empty\n",
    "    if not train_df.empty and not val_df.empty and not test_df.empty:\n",
    "        assert train_df.index.max() < val_df.index.min(), \"Train and Validation sets overlap!\"\n",
    "        assert val_df.index.max() < test_df.index.min(), \"Validation and Test sets overlap!\"\n",
    "        print(\"Data splits are chronological and non-overlapping.\")\n",
    "    elif train_df.empty or val_df.empty or test_df.empty:\n",
    "        print(\"Warning: One or more data splits are empty, cannot verify overlap fully.\")\n",
    "else:\n",
    "    print(\"Combined DataFrame is empty. Skipping data splitting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b50c30-7d3e-4c65-8b18-c218464c5a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scaling Numerical Features ---\n",
      "Scaling complete.\n",
      "\n",
      "Scaler saved to: D:\\.Study\\projects\\EnergyForecasting\\data\\model_ready\\feature_scaler_multiyear.joblib\n"
     ]
    }
   ],
   "source": [
    "train_scaled_df, val_scaled_df, test_scaled_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame() # Initialize\n",
    "scaler = None # Initialize\n",
    "\n",
    "if not train_df.empty and not val_df.empty and not test_df.empty : # Ensure all splits exist\n",
    "    print(\"\\n--- Scaling Numerical Features ---\")\n",
    "    \n",
    "    # Use PJM_ZONES_UPPER from Cell 1 for consistency\n",
    "    targets = [target_column_template.replace('{ZONE}', zone) for zone in PJM_ZONES_UPPER]\n",
    "    \n",
    "    # For simplicity, scale all columns. If you had true categorical string columns\n",
    "    # (not 0/1 like 'IsDay' or WMO codes), they'd need one-hot encoding first and exclusion here.\n",
    "    # Your current features (load, weather vars, time features like hour_sin) are all numeric.\n",
    "    feature_columns_to_scale = train_df.columns.tolist()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler ONLY on the training data\n",
    "    train_scaled_data = scaler.fit_transform(train_df[feature_columns_to_scale])\n",
    "    \n",
    "    # Transform validation and test data using the FITTED scaler\n",
    "    val_scaled_data = scaler.transform(val_df[feature_columns_to_scale])\n",
    "    test_scaled_data = scaler.transform(test_df[feature_columns_to_scale])\n",
    "    \n",
    "    train_scaled_df = pd.DataFrame(train_scaled_data, columns=feature_columns_to_scale, index=train_df.index)\n",
    "    val_scaled_df = pd.DataFrame(val_scaled_data, columns=feature_columns_to_scale, index=val_df.index)\n",
    "    test_scaled_df = pd.DataFrame(test_scaled_data, columns=feature_columns_to_scale, index=test_df.index)\n",
    "    \n",
    "    print(\"Scaling complete.\")\n",
    "    # print(\"\\nScaled Training Data (sample):\")\n",
    "    # print(train_scaled_df.head(3))\n",
    "    # print(\"\\nMean of scaled training data (sample of first 5 columns, should be ~0):\")\n",
    "    # print(train_scaled_df.mean().head())\n",
    "    # print(\"\\nStd Dev of scaled training data (sample of first 5 columns, should be ~1):\")\n",
    "    # print(train_scaled_df.std().head())\n",
    "\n",
    "    if not os.path.exists(model_ready_data_dir):\n",
    "        os.makedirs(model_ready_data_dir)\n",
    "        print(f\"Created directory: {model_ready_data_dir}\")\n",
    "        \n",
    "    scaler_filename = 'feature_scaler_multiyear.joblib' # New name for new data\n",
    "    scaler_path = os.path.join(model_ready_data_dir, scaler_filename)\n",
    "    joblib.dump(scaler, scaler_path)\n",
    "    print(f\"\\nScaler saved to: {scaler_path}\")\n",
    "    \n",
    "else:\n",
    "    print(\"One or more data splits (train_df, val_df, test_df) are empty. Skipping scaling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336f8ad1-bdc8-42a6-b682-1ee9094fe3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Saving Prepared DataFrames ---\n",
      "Scaled train, validation, and test DataFrames saved with 'multiyear' suffix.\n",
      "  Train: D:\\.Study\\projects\\EnergyForecasting\\data\\model_ready\\train_scaled_multiyear.csv\n",
      "  Val:   D:\\.Study\\projects\\EnergyForecasting\\data\\model_ready\\val_scaled_multiyear.csv\n",
      "  Test:  D:\\.Study\\projects\\EnergyForecasting\\data\\model_ready\\test_scaled_multiyear.csv\n"
     ]
    }
   ],
   "source": [
    "if not train_scaled_df.empty and not val_scaled_df.empty and not test_scaled_df.empty:\n",
    "    print(\"\\n--- Saving Prepared DataFrames ---\")\n",
    "    \n",
    "    if not os.path.exists(model_ready_data_dir): # Should have been created in cell 6 if needed\n",
    "        os.makedirs(model_ready_data_dir)\n",
    "        # print(f\"Created directory: {model_ready_data_dir}\") # Already printed if created\n",
    "\n",
    "    # Use new filenames for the scaled data from multi-year processing\n",
    "    train_scaled_df.to_csv(os.path.join(model_ready_data_dir, 'train_scaled_multiyear.csv'))\n",
    "    val_scaled_df.to_csv(os.path.join(model_ready_data_dir, 'val_scaled_multiyear.csv'))\n",
    "    test_scaled_df.to_csv(os.path.join(model_ready_data_dir, 'test_scaled_multiyear.csv'))\n",
    "    \n",
    "    print(\"Scaled train, validation, and test DataFrames saved with 'multiyear' suffix.\")\n",
    "    print(f\"  Train: {os.path.join(model_ready_data_dir, 'train_scaled_multiyear.csv')}\")\n",
    "    print(f\"  Val:   {os.path.join(model_ready_data_dir, 'val_scaled_multiyear.csv')}\")\n",
    "    print(f\"  Test:  {os.path.join(model_ready_data_dir, 'test_scaled_multiyear.csv')}\")\n",
    "\n",
    "else:\n",
    "    print(\"Scaled data (train_scaled_df, val_scaled_df, or test_scaled_df) is empty. Nothing to save.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd3e8a-13d8-4995-bce4-c55aecb07c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
